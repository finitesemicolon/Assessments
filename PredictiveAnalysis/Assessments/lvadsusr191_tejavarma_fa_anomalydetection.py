# -*- coding: utf-8 -*-
"""LVADSUSR191_TejaVarma_fa_AnomalyDetection

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jhH6vN5MiKr8ZkfKnxBmhWGDGegoweuJ
"""

import numpy as np
import pandas as pd
import re
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
from sklearn.tree import export_graphviz
from sklearn.impute import KNNImputer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from imblearn.over_sampling import SMOTE
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier, IsolationForest
from sklearn.model_selection import cross_val_score
from xgboost import XGBClassifier
import xgboost
from lightgbm import LGBMClassifier
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import confusion_matrix, classification_report, f1_score, roc_curve, roc_auc_score, precision_recall_curve, auc, r2_score, mean_squared_error, accuracy_score, recall_score, silhouette_score, silhouette_samples
warnings.filterwarnings('ignore')

df = pd.read_csv('https://raw.githubusercontent.com/Deepsphere-AI/LVA-Batch5-Assessment/main/anomaly_train.csv')

df.head()

df.info()

df.shape

df.isnull().sum() # there are no null values in the dataset

df.duplicated().sum() # there are no duplicate data either

df.dtypes

# encoding location

le = LabelEncoder()
df['Location'] = le.fit_transform(df['Location'])

df.dtypes

#defining x and y in the dataset based on the given numerical columns
X = df[['Amount','Location']] # only taking the amount of money and time into consideration
# location can also be considered for the anomaly detection but it should be encoded into numerical before even performing the model transformation

model = IsolationForest(n_estimators=100,contamination=0.1)
model.fit(X)

y_pred = model.predict(X)

df['anomaly_score'] = model.decision_function(X)
df['anomaly'] = y_pred

df.head()

anomalies = df.loc[df['anomaly_score'] < 0]

anomalies

anomalies.shape # there are around 30 anomalies in the dataset

plt.scatter(df['Amount'],df['anomaly_score'],label='normal')
plt.scatter(anomalies['Amount'],anomalies['anomaly_score'],label='anomaly')
plt.xlabel('Amount')
plt.ylabel('anomaly_score')
plt.legend()
plt.show()

df['is_anomaly'] = df['anomaly'].map({-1:"yes",1:"no"})  # mapping the values of is_anomaly or not

df