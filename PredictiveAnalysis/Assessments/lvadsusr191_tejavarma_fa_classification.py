# -*- coding: utf-8 -*-
"""LVADSUSR191_TejaVarma_fa_Classification

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BamYCHipzjIAEHfki4Ig1lDgw8ug0sah
"""

import numpy as np
import pandas as pd
import re
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
from sklearn.tree import export_graphviz
from sklearn.impute import KNNImputer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from imblearn.over_sampling import SMOTE
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier, IsolationForest
from sklearn.model_selection import cross_val_score
from xgboost import XGBClassifier
import xgboost
from lightgbm import LGBMClassifier
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import confusion_matrix, classification_report, f1_score, roc_curve, roc_auc_score, precision_recall_curve, auc, r2_score, mean_squared_error, accuracy_score, recall_score, silhouette_score, silhouette_samples
warnings.filterwarnings('ignore')

df = pd.read_csv('https://raw.githubusercontent.com/Deepsphere-AI/LVA-Batch5-Assessment/main/penguins_classification.csv')

df.head()

df.info()

df.isnull().sum() # checking for null values --- there are 8 null values in bill_length_mm

df.duplicated().sum() # checking for duplicate values --- none found

# replacing null vlaues with the median of them

df['bill_depth_mm'] = df['bill_depth_mm'].fillna(df['bill_depth_mm'].median)

df.isnull().sum() # null values are replaced in the data

#dropping species column

df = df.drop('species',axis= 1,inplace = True)

# correlation matrix

corr_mat = df.corr()
print(corr_mat)

sns.heatmap(corr_mat)

# predicting the x_train and y_train values
x = df[df.drop(['bill_length_mm','bill_depth_mm','flipper_length_mm'],axis=1 , inplace = True)]
y = df[['bill_length_mm','bill_depth_mm','flipper_length_mm']]

x_train,x_test,y_train,y_test = train_test_split(x,y, test_size = 0.2)

#
model = RandomForestClassifier()
y_pred = model.fit_transform(df)

# metrics
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

print("Confusion Matrix: ",
confusion_matrix(y_test, y_pred))