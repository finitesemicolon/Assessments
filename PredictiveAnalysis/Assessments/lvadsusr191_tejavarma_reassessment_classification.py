# -*- coding: utf-8 -*-
"""LVADSUSR191_TejaVarma_Classification

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Eme59D0uFIBfX1gDI5RK0eAzpAX5YY4h
"""

#importing libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import MinMaxScaler,StandardScaler , LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

from numpy import loadtxt
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score , precision_score , recall_score , f1_score , confusion_matrix

df =pd.read_csv('https://raw.githubusercontent.com/Deepsphere-AI/LVA-Batch5-Assessment/main/mushroom.csv')
# data loading

df.head()

df.info()

df.isna().sum()
# null values found

df.duplicated().sum()
# 303 dupllicates found

df = df.drop_duplicates()
#dropping duplicsates

df.shape

# filling the null values based on the numerical distribution
for column in df.select_dtypes(include=['int64','float64']).columns:
  plt.figure(figsize=(10,5))
  sns.histplot(df[column])

# stem color is left skewed so we are filling with the median values
df['stem-color'] = df['stem-color'].fillna(df['stem-color'].median())

df.isna().sum()

#check for outliers
for c in df.select_dtypes(include=['int64','float64']).columns:
  plt.figure(figsize=(10,5))
  sns.boxplot(df[c])

# outlier treatment (clipping and capping the outliers to upper and lower bounds)
for c in df.select_dtypes(include=['int64','float64']).columns:
  q1 = df[c].quantile(0.25)
  q3 = df[c].quantile(0.75)
  iqr = q3-q1
  lwr = q1-1.5*iqr
  upr = q3+1.5*iqr
  df.loc[df[c]>upr,c]=upr
  df.loc[df[c]<lwr,c]=lwr

# checking for any outliers again
for c in df.select_dtypes(include=['int64','float64']).columns:
  plt.figure(figsize=(10,5))
  sns.boxplot(df[c])

# assigning correlation matrix and heat map
numeric = df.select_dtypes(include = ['int64','float64']).columns
heat = df[numeric].corr()
plt.figure(figsize=(15,10))
sns.heatmap(heat,annot = True)

# Encoding, here all the columns are numerical so there is no need of encoding in it

# label_encoder = LabelEncoder()

# for col in df.columns:
#     if df[col].dtype == 'object':  # Check if the column contains categorical data
#         df[col] = label_encoder.fit_transform(df[col])

df.head()

df.columns

#feature selection
x = df.drop(columns=['class'])
y = df['class']
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3)

#scaling
scaler=MinMaxScaler()
x_train=scaler.fit_transform(x_train)
x_test=scaler.transform(x_test)

#model fit

model = RandomForestClassifier()
model.fit(x_train,y_train)
y_pred = model.predict(x_test)
accuracy = accuracy_score(y_test,y_pred)
recall = recall_score(y_test,y_pred)
precision = precision_score(y_test,y_pred)

#metrics
conf = confusion_matrix(y_test,y_pred)
F1 = f1_score(y_test,y_pred)
print("Accuracy Score: ", accuracy)
print("recall Score: ", recall)
print("precision Score: ", precision)
print("Confusion Matrix: ", conf)


model = LogisticRegression()
model.fit(x_train,y_train)
y_pred = model.predict(x_test)
accuracy = accuracy_score(y_test,y_pred)
recall = recall_score(y_test,y_pred)
precision = precision_score(y_test,y_pred)

#metrics
conf = confusion_matrix(y_test,y_pred)
F1 = f1_score(y_test,y_pred)
print("Accuracy Score: ", accuracy)
print("recall Score: ", recall)
print("precision Score: ", precision)
print("Confusion Matrix: ", conf)

#RandomForest has better accuracy metrics than Logistic Regression
#RandomForest : Accuracy Score:  0.9903225806451613
#LogisticRegression : Accuracy Score:  0.6282258064516129
# So it is considered as a better model than the rest